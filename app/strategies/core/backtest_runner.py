# íŒŒì¼ëª…: app/strategies/core/backtest_runner.py
"""
ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰ê¸°
ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰ì„ ê´€ë¦¬í•˜ê³  ê²°ê³¼ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.
"""

import asyncio
import json
import pickle
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
from typing import Dict, List, Optional, Callable, Any, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict
from pathlib import Path
import logging
from enum import Enum

from .backtest_engine import BacktestEngine, BacktestConfig, BacktestResults
from .performance_analyzer import PerformanceAnalyzer, AnalysisType
from .strategy_base import StrategyBase

logger = logging.getLogger(__name__)


class ExecutionMode(Enum):
    """ì‹¤í–‰ ëª¨ë“œ"""
    SINGLE = "SINGLE"           # ë‹¨ì¼ ë°±í…ŒìŠ¤íŠ¸
    BATCH = "BATCH"             # ë°°ì¹˜ ì‹¤í–‰
    OPTIMIZATION = "OPTIMIZATION"  # íŒŒë¼ë¯¸í„° ìµœì í™”
    MONTE_CARLO = "MONTE_CARLO"    # ëª¬í…Œì¹´ë¥¼ë¡œ ì‹œë®¬ë ˆì´ì…˜


class OptimizationMethod(Enum):
    """ìµœì í™” ë°©ë²•"""
    GRID_SEARCH = "GRID_SEARCH"
    RANDOM_SEARCH = "RANDOM_SEARCH"
    GENETIC_ALGORITHM = "GENETIC_ALGORITHM"
    BAYESIAN = "BAYESIAN"


@dataclass
class BacktestJob:
    """ë°±í…ŒìŠ¤íŠ¸ ì‘ì—…"""
    job_id: str
    strategy_config: Dict[str, Any]
    backtest_config: BacktestConfig
    market_data: List[Dict]
    metadata: Dict[str, Any] = None
    status: str = "PENDING"
    created_at: datetime = None
    started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None
    error_message: Optional[str] = None


@dataclass
class OptimizationConfig:
    """ìµœì í™” ì„¤ì •"""
    method: OptimizationMethod = OptimizationMethod.GRID_SEARCH
    objective_function: str = "sharpe_ratio"  # sharpe_ratio, total_return, calmar_ratio
    maximize: bool = True
    max_iterations: int = 100
    population_size: int = 50  # GAìš©
    mutation_rate: float = 0.1  # GAìš©
    crossover_rate: float = 0.8  # GAìš©
    early_stopping_patience: int = 10
    early_stopping_min_delta: float = 0.001


@dataclass
class BacktestSuite:
    """ë°±í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸"""
    suite_id: str
    name: str
    description: str
    jobs: List[BacktestJob]
    results: Dict[str, BacktestResults] = None
    summary: Dict[str, Any] = None
    created_at: datetime = None
    completed_at: Optional[datetime] = None


class BacktestRunner:
    """ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰ê¸°"""
    
    def __init__(self, max_workers: int = 4, results_dir: str = "backtest_results"):
        self.max_workers = max_workers
        self.results_dir = Path(results_dir)
        self.results_dir.mkdir(exist_ok=True)
        
        self.analyzer = PerformanceAnalyzer()
        self.active_jobs: Dict[str, BacktestJob] = {}
        self.completed_jobs: Dict[str, BacktestResults] = {}
        
        # ì§„í–‰ìƒí™© ì½œë°±
        self.progress_callbacks: List[Callable] = []
        
        logger.info(f"ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰ê¸° ì´ˆê¸°í™”: {max_workers} workers")
    
    def add_progress_callback(self, callback: Callable[[str, float], None]):
        """ì§„í–‰ìƒí™© ì½œë°± ì¶”ê°€"""
        self.progress_callbacks.append(callback)
    
    def _notify_progress(self, job_id: str, progress: float):
        """ì§„í–‰ìƒí™© ì•Œë¦¼"""
        for callback in self.progress_callbacks:
            try:
                callback(job_id, progress)
            except Exception as e:
                logger.warning(f"ì§„í–‰ìƒí™© ì½œë°± ì˜¤ë¥˜: {e}")
    
    async def run_single_backtest(self, 
                                strategy: StrategyBase,
                                config: BacktestConfig,
                                market_data: List[Dict],
                                job_id: Optional[str] = None) -> BacktestResults:
        """ë‹¨ì¼ ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        
        if not job_id:
            job_id = f"single_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        logger.info(f"ë‹¨ì¼ ë°±í…ŒìŠ¤íŠ¸ ì‹œì‘: {job_id}")
        
        try:
            # ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰
            engine = BacktestEngine(strategy, config)
            results = await asyncio.get_event_loop().run_in_executor(
                None, engine.run, market_data
            )
            
            # ì„±ê³¼ ë¶„ì„
            analysis = self.analyzer.analyze(results, AnalysisType.DETAILED)
            results.analysis = analysis
            
            # ê²°ê³¼ ì €ì¥
            await self._save_results(job_id, results)
            
            logger.info(f"ë‹¨ì¼ ë°±í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {job_id}")
            return results
            
        except Exception as e:
            logger.error(f"ë‹¨ì¼ ë°±í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ ({job_id}): {str(e)}")
            raise
    
    async def run_batch_backtest(self, 
                                jobs: List[BacktestJob],
                                execution_mode: ExecutionMode = ExecutionMode.BATCH) -> Dict[str, BacktestResults]:
        """ë°°ì¹˜ ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        
        logger.info(f"ë°°ì¹˜ ë°±í…ŒìŠ¤íŠ¸ ì‹œì‘: {len(jobs)}ê°œ ì‘ì—…")
        
        results = {}
        
        if execution_mode == ExecutionMode.BATCH:
            # ë³‘ë ¬ ì‹¤í–‰
            tasks = []
            for job in jobs:
                task = self._execute_single_job(job)
                tasks.append(task)
            
            completed_results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for job, result in zip(jobs, completed_results):
                if isinstance(result, Exception):
                    logger.error(f"ì‘ì—… ì‹¤íŒ¨ ({job.job_id}): {result}")
                else:
                    results[job.job_id] = result
        
        logger.info(f"ë°°ì¹˜ ë°±í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {len(results)}ê°œ ì„±ê³µ")
        return results
    
    async def run_parameter_optimization(self,
                                       strategy_class: type,
                                       base_config: BacktestConfig,
                                       market_data: List[Dict],
                                       parameter_ranges: Dict[str, List],
                                       optimization_config: OptimizationConfig) -> Dict[str, Any]:
        """íŒŒë¼ë¯¸í„° ìµœì í™” ì‹¤í–‰"""
        
        logger.info(f"íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘: {optimization_config.method.value}")
        
        if optimization_config.method == OptimizationMethod.GRID_SEARCH:
            return await self._grid_search_optimization(
                strategy_class, base_config, market_data, parameter_ranges, optimization_config
            )
        elif optimization_config.method == OptimizationMethod.RANDOM_SEARCH:
            return await self._random_search_optimization(
                strategy_class, base_config, market_data, parameter_ranges, optimization_config
            )
        else:
            raise NotImplementedError(f"ìµœì í™” ë°©ë²• ë¯¸êµ¬í˜„: {optimization_config.method}")
    
    async def _grid_search_optimization(self,
                                      strategy_class: type,
                                      base_config: BacktestConfig,
                                      market_data: List[Dict],
                                      parameter_ranges: Dict[str, List],
                                      optimization_config: OptimizationConfig) -> Dict[str, Any]:
        """ê·¸ë¦¬ë“œ ì„œì¹˜ ìµœì í™”"""
        
        import itertools
        
        # íŒŒë¼ë¯¸í„° ì¡°í•© ìƒì„±
        param_names = list(parameter_ranges.keys())
        param_values = list(parameter_ranges.values())
        param_combinations = list(itertools.product(*param_values))
        
        logger.info(f"ê·¸ë¦¬ë“œ ì„œì¹˜: {len(param_combinations)}ê°œ ì¡°í•© í…ŒìŠ¤íŠ¸")
        
        best_score = float('-inf') if optimization_config.maximize else float('inf')
        best_params = None
        best_results = None
        
        results_history = []
        
        for i, param_combo in enumerate(param_combinations):
            # íŒŒë¼ë¯¸í„° ì„¤ì •
            params = dict(zip(param_names, param_combo))
            
            try:
                # ì „ëµ ìƒì„±
                strategy = strategy_class(**params)
                
                # ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰
                engine = BacktestEngine(strategy, base_config)
                results = await asyncio.get_event_loop().run_in_executor(
                    None, engine.run, market_data
                )
                
                # ëª©ì  í•¨ìˆ˜ ê³„ì‚°
                score = self._calculate_objective_score(results, optimization_config.objective_function)
                
                # ìµœì í•´ ì—…ë°ì´íŠ¸
                is_better = (score > best_score) if optimization_config.maximize else (score < best_score)
                
                if is_better:
                    best_score = score
                    best_params = params.copy()
                    best_results = results
                
                results_history.append({
                    'iteration': i + 1,
                    'parameters': params,
                    'score': score,
                    'is_best': is_better
                })
                
                # ì§„í–‰ìƒí™© ì•Œë¦¼
                progress = (i + 1) / len(param_combinations)
                self._notify_progress(f"optimization", progress)
                
                logger.debug(f"ë°˜ë³µ {i+1}/{len(param_combinations)}: "
                           f"ì ìˆ˜={score:.4f}, ìµœê³ ={best_score:.4f}")
                
            except Exception as e:
                logger.warning(f"íŒŒë¼ë¯¸í„° ì¡°í•© ì‹¤íŒ¨ ({params}): {e}")
                continue
        
        return {
            'method': 'grid_search',
            'best_parameters': best_params,
            'best_score': best_score,
            'best_results': best_results,
            'total_iterations': len(param_combinations),
            'successful_iterations': len(results_history),
            'results_history': results_history
        }
    
    async def _random_search_optimization(self,
                                        strategy_class: type,
                                        base_config: BacktestConfig,
                                        market_data: List[Dict],
                                        parameter_ranges: Dict[str, List],
                                        optimization_config: OptimizationConfig) -> Dict[str, Any]:
        """ëœë¤ ì„œì¹˜ ìµœì í™”"""
        
        import random
        
        logger.info(f"ëœë¤ ì„œì¹˜: {optimization_config.max_iterations}íšŒ ë°˜ë³µ")
        
        best_score = float('-inf') if optimization_config.maximize else float('inf')
        best_params = None
        best_results = None
        
        results_history = []
        no_improvement_count = 0
        
        for iteration in range(optimization_config.max_iterations):
            # ëœë¤ íŒŒë¼ë¯¸í„° ìƒì„±
            params = {}
            for param_name, param_range in parameter_ranges.items():
                params[param_name] = random.choice(param_range)
            
            try:
                # ì „ëµ ìƒì„±
                strategy = strategy_class(**params)
                
                # ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰
                engine = BacktestEngine(strategy, base_config)
                results = await asyncio.get_event_loop().run_in_executor(
                    None, engine.run, market_data
                )
                
                # ëª©ì  í•¨ìˆ˜ ê³„ì‚°
                score = self._calculate_objective_score(results, optimization_config.objective_function)
                
                # ìµœì í•´ ì—…ë°ì´íŠ¸
                is_better = (score > best_score) if optimization_config.maximize else (score < best_score)
                
                if is_better:
                    improvement = abs(score - best_score)
                    if improvement >= optimization_config.early_stopping_min_delta:
                        no_improvement_count = 0
                    else:
                        no_improvement_count += 1
                    
                    best_score = score
                    best_params = params.copy()
                    best_results = results
                else:
                    no_improvement_count += 1
                
                results_history.append({
                    'iteration': iteration + 1,
                    'parameters': params,
                    'score': score,
                    'is_best': is_better
                })
                
                # ì¡°ê¸° ì¢…ë£Œ í™•ì¸
                if no_improvement_count >= optimization_config.early_stopping_patience:
                    logger.info(f"ì¡°ê¸° ì¢…ë£Œ: {no_improvement_count}íšŒ ê°œì„  ì—†ìŒ")
                    break
                
                # ì§„í–‰ìƒí™© ì•Œë¦¼
                progress = (iteration + 1) / optimization_config.max_iterations
                self._notify_progress(f"optimization", progress)
                
            except Exception as e:
                logger.warning(f"ëœë¤ íŒŒë¼ë¯¸í„° ì‹¤íŒ¨ ({params}): {e}")
                continue
        
        return {
            'method': 'random_search',
            'best_parameters': best_params,
            'best_score': best_score,
            'best_results': best_results,
            'total_iterations': len(results_history),
            'early_stopped': no_improvement_count >= optimization_config.early_stopping_patience,
            'results_history': results_history
        }
    
    def _calculate_objective_score(self, results: BacktestResults, objective_function: str) -> float:
        """ëª©ì  í•¨ìˆ˜ ì ìˆ˜ ê³„ì‚°"""
        
        if objective_function == "sharpe_ratio":
            return results.sharpe_ratio
        elif objective_function == "total_return":
            return results.total_return
        elif objective_function == "calmar_ratio":
            return results.calmar_ratio
        elif objective_function == "profit_factor":
            return results.profit_factor if hasattr(results, 'profit_factor') else 0
        elif objective_function == "win_rate":
            return results.win_rate
        elif objective_function == "max_drawdown":
            return -results.max_drawdown  # ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
        else:
            logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” ëª©ì  í•¨ìˆ˜: {objective_function}")
            return 0.0
    
    async def _execute_single_job(self, job: BacktestJob) -> BacktestResults:
        """ë‹¨ì¼ ì‘ì—… ì‹¤í–‰"""
        
        job.status = "RUNNING"
        job.started_at = datetime.now()
        
        try:
            # ì „ëµ ìƒì„± (ë™ì )
            strategy_config = job.strategy_config
            strategy_class_name = strategy_config.pop('class_name', 'ScalpingStrategy')
            
            # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í™” - ì‹¤ì œë¡œëŠ” ë™ì  ì„í¬íŠ¸ í•„ìš”
            from ..plugins.scalping_strategy import create_scalping_strategy
            strategy = create_scalping_strategy()
            
            # ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰
            engine = BacktestEngine(strategy, job.backtest_config)
            results = await asyncio.get_event_loop().run_in_executor(
                None, engine.run, job.market_data
            )
            
            job.status = "COMPLETED"
            job.completed_at = datetime.now()
            
            return results
            
        except Exception as e:
            job.status = "FAILED"
            job.error_message = str(e)
            job.completed_at = datetime.now()
            logger.error(f"ì‘ì—… ì‹¤í–‰ ì‹¤íŒ¨ ({job.job_id}): {e}")
            raise
    
    async def _save_results(self, job_id: str, results: BacktestResults):
        """ê²°ê³¼ ì €ì¥"""
        
        try:
            # JSON í˜•ì‹ìœ¼ë¡œ ì €ì¥ (ê¸°ë³¸ ì •ë³´)
            json_path = self.results_dir / f"{job_id}.json"
            json_data = {
                'job_id': job_id,
                'config': asdict(results.config),
                'total_return': results.total_return,
                'sharpe_ratio': results.sharpe_ratio,
                'max_drawdown': results.max_drawdown,
                'total_trades': len(results.trades),
                'created_at': datetime.now().isoformat()
            }
            
            with open(json_path, 'w') as f:
                json.dump(json_data, f, indent=2)
            
            # Pickle í˜•ì‹ìœ¼ë¡œ ì „ì²´ ê²°ê³¼ ì €ì¥
            pickle_path = self.results_dir / f"{job_id}.pkl"
            with open(pickle_path, 'wb') as f:
                pickle.dump(results, f)
            
            logger.debug(f"ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {job_id}")
            
        except Exception as e:
            logger.error(f"ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨ ({job_id}): {e}")
    
    async def load_results(self, job_id: str) -> Optional[BacktestResults]:
        """ê²°ê³¼ ë¡œë“œ"""
        
        pickle_path = self.results_dir / f"{job_id}.pkl"
        
        if not pickle_path.exists():
            return None
        
        try:
            with open(pickle_path, 'rb') as f:
                results = pickle.load(f)
            
            logger.debug(f"ê²°ê³¼ ë¡œë“œ ì™„ë£Œ: {job_id}")
            return results
            
        except Exception as e:
            logger.error(f"ê²°ê³¼ ë¡œë“œ ì‹¤íŒ¨ ({job_id}): {e}")
            return None
    
    def list_saved_results(self) -> List[Dict[str, Any]]:
        """ì €ì¥ëœ ê²°ê³¼ ëª©ë¡"""
        
        results = []
        
        for json_file in self.results_dir.glob("*.json"):
            try:
                with open(json_file, 'r') as f:
                    data = json.load(f)
                results.append(data)
            except Exception as e:
                logger.warning(f"ê²°ê³¼ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ ({json_file}): {e}")
        
        return sorted(results, key=lambda x: x.get('created_at', ''), reverse=True)
    
    def cleanup_old_results(self, days_to_keep: int = 30):
        """ì˜¤ë˜ëœ ê²°ê³¼ ì •ë¦¬"""
        
        cutoff_date = datetime.now() - timedelta(days=days_to_keep)
        cleaned_count = 0
        
        for file_path in self.results_dir.glob("*"):
            if file_path.is_file():
                file_time = datetime.fromtimestamp(file_path.stat().st_mtime)
                if file_time < cutoff_date:
                    file_path.unlink()
                    cleaned_count += 1
        
        logger.info(f"ì˜¤ë˜ëœ ê²°ê³¼ {cleaned_count}ê°œ ì •ë¦¬ ì™„ë£Œ")
    
    def create_comparison_report(self, job_ids: List[str]) -> Dict[str, Any]:
        """ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„±"""
        
        comparison_data = []
        
        for job_id in job_ids:
            json_path = self.results_dir / f"{job_id}.json"
            if json_path.exists():
                with open(json_path, 'r') as f:
                    data = json.load(f)
                comparison_data.append(data)
        
        if not comparison_data:
            return {'error': 'ë¹„êµí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤'}
        
        # ë¹„êµ ë¶„ì„
        metrics = ['total_return', 'sharpe_ratio', 'max_drawdown', 'total_trades']
        comparison = {}
        
        for metric in metrics:
            values = [data.get(metric, 0) for data in comparison_data]
            comparison[metric] = {
                'values': values,
                'best': max(values),
                'worst': min(values),
                'average': sum(values) / len(values),
                'best_job': job_ids[values.index(max(values))]
            }
        
        return {
            'compared_jobs': job_ids,
            'comparison': comparison,
            'job_data': comparison_data
        }


def create_sample_optimization_config() -> OptimizationConfig:
    """ìƒ˜í”Œ ìµœì í™” ì„¤ì • ìƒì„±"""
    
    return OptimizationConfig(
        method=OptimizationMethod.RANDOM_SEARCH,
        objective_function="sharpe_ratio",
        maximize=True,
        max_iterations=50,
        early_stopping_patience=10
    )


def create_sample_parameter_ranges() -> Dict[str, List]:
    """ìƒ˜í”Œ íŒŒë¼ë¯¸í„° ë²”ìœ„ ìƒì„±"""
    
    return {
        'target_profit': [0.005, 0.008, 0.010, 0.012, 0.015],
        'stop_loss': [0.002, 0.003, 0.004, 0.005],
        'rsi_oversold': [20, 25, 30, 35],
        'rsi_overbought': [65, 70, 75, 80],
        'max_daily_trades': [20, 30, 40, 50]
    }


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    import asyncio
    from ..plugins.scalping_strategy import create_scalping_strategy
    from .backtest_engine import create_sample_market_data
    
    async def main():
        # ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰ê¸° ìƒì„±
        runner = BacktestRunner(max_workers=2)
        
        # ì „ëµ ìƒì„±
        strategy = create_scalping_strategy()
        
        # ë°±í…ŒìŠ¤íŠ¸ ì„¤ì •
        config = BacktestConfig(
            start_date="2024-01-01T00:00:00",
            end_date="2024-01-31T23:59:59",
            initial_balance=10000
        )
        
        # ìƒ˜í”Œ ë°ì´í„°
        market_data = create_sample_market_data(days=30)
        
        # ë‹¨ì¼ ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        results = await runner.run_single_backtest(strategy, config, market_data)
        
        print(f"ğŸ¯ ë°±í…ŒìŠ¤íŠ¸ ì™„ë£Œ:")
        print(f"ì´ ìˆ˜ìµë¥ : {results.total_return:.2f}%")
        print(f"ìƒ¤í”„ ë¹„ìœ¨: {results.sharpe_ratio:.2f}")
        print(f"ì´ ê±°ë˜: {len(results.trades)}ê±´")
    
    # ì‹¤í–‰
    asyncio.run(main())